{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":11,"metadata":{"id":"ZVjU7tKybGTB","executionInfo":{"status":"ok","timestamp":1690039674556,"user_tz":-420,"elapsed":287,"user":{"displayName":"Hà Dung - Hoàng Anh","userId":"11283816701094956411"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.init as init\n","import csv\n","import numpy as np\n","import os\n","import time\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","import pandas as pd"]},{"cell_type":"code","source":["class ChebConv(nn.Module):\n","    \"\"\"\n","    The ChebNet convolution operation.\n","    :param in_c: int, number of input channels.\n","    :param out_c: int, number of output channels.\n","    :param K: int, the order of Chebyshev Polynomial.\n","    \"\"\"\n","    def __init__(self, in_c, out_c, K, bias=True, normalize=True):\n","        super(ChebConv, self).__init__()\n","        self.normalize = normalize\n","\n","        self.weight = nn.Parameter(torch.Tensor(K + 1, 1, in_c, out_c))  # [K+1, 1, in_c, out_c]\n","        init.xavier_normal_(self.weight)\n","\n","        if bias:\n","            self.bias = nn.Parameter(torch.Tensor(1, 1, out_c))\n","            init.zeros_(self.bias)\n","        else:\n","            self.register_parameter(\"bias\", None)\n","\n","        self.K = K + 1\n","\n","    def forward(self, inputs, graph):\n","        \"\"\"\n","        :param inputs: the input data, [B, N, C]\n","        :param graph: the graph structure, [N, N]\n","        :return: convolution result, [B, N, D]\n","        \"\"\"\n","        L = ChebConv.get_laplacian(graph, self.normalize)  # [N, N]\n","        mul_L = self.cheb_polynomial(L).unsqueeze(1)   # [K, 1, N, N]\n","\n","        result = torch.matmul(mul_L, inputs)  # [K, B, N, C]\n","        result = torch.matmul(result, self.weight)  # [K, B, N, D]\n","        result = torch.sum(result, dim=0) + self.bias  # [B, N, D]\n","\n","        return result\n","\n","    def cheb_polynomial(self, laplacian):\n","        \"\"\"\n","        Compute the Chebyshev Polynomial, according to the graph laplacian.\n","        :param laplacian: the graph laplacian, [N, N].\n","        :return: the multi order Chebyshev laplacian, [K, N, N].\n","        \"\"\"\n","        N = laplacian.size(0)  # [N, N]\n","        multi_order_laplacian = torch.zeros([self.K, N, N], device=laplacian.device, dtype=torch.float)  # [K, N, N]\n","        multi_order_laplacian[0] = torch.eye(N, device=laplacian.device, dtype=torch.float)\n","\n","        if self.K == 1:\n","            return multi_order_laplacian\n","        else:\n","            multi_order_laplacian[1] = laplacian\n","            if self.K == 2:\n","                return multi_order_laplacian\n","            else:\n","                for k in range(2, self.K):\n","                    multi_order_laplacian[k] = 2 * torch.mm(laplacian, multi_order_laplacian[k-1]) - \\\n","                                               multi_order_laplacian[k-2]\n","\n","        return multi_order_laplacian\n","\n","    @staticmethod\n","    def get_laplacian(graph, normalize):\n","        \"\"\"\n","        return the laplacian of the graph.\n","        :param graph: the graph structure without self loop, [N, N].\n","        :param normalize: whether to used the normalized laplacian.\n","        :return: graph laplacian.\n","        \"\"\"\n","        if normalize:\n","\n","            D = torch.diag(torch.sum(graph, dim=-1) ** (-1 / 2))\n","            L = torch.eye(graph.size(0), device=graph.device, dtype=graph.dtype) - torch.mm(torch.mm(D, graph), D)\n","        else:\n","            D = torch.diag(torch.sum(graph, dim=-1))\n","            L = D - graph\n","        return L\n","\n","\n","class ChebNet(nn.Module):\n","    def __init__(self, in_c, hid_c, out_c, K):\n","        \"\"\"\n","        :param in_c: int, number of input channels.\n","        :param hid_c: int, number of hidden channels.\n","        :param out_c: int, number of output channels.\n","        :param K:\n","        \"\"\"\n","        super(ChebNet, self).__init__()\n","        self.conv1 = ChebConv(in_c=in_c, out_c=hid_c, K=K)\n","        self.conv2 = ChebConv(in_c=hid_c, out_c=out_c, K=K)\n","        self.act = nn.ReLU()\n","\n","    def forward(self, data, device):\n","        graph_data = data[\"graph\"].to(device)[0]  # [N, N]\n","        flow_x = data[\"flow_x\"].to(device)  # [B, N, H, D]\n","\n","        B, N = flow_x.size(0), flow_x.size(1)\n","\n","        flow_x = flow_x.view(B, N, -1)  # [B, N, H*D]\n","\n","        output_1 = self.act(self.conv1(flow_x, graph_data))\n","        output_2 = self.act(self.conv2(output_1, graph_data))\n","\n","        return output_2.unsqueeze(2)\n","\n","\n","\n","\n","\n","\n","class GCN(nn.Module):\n","    def __init__(self, in_c, hid_c, out_c):\n","        super(GCN, self).__init__()\n","        self.linear_1 = nn.Linear(in_c, hid_c)\n","        self.linear_2 = nn.Linear(hid_c, out_c)\n","        self.act = nn.ReLU()\n","\n","    def forward(self, data, device):\n","        graph_data = data[\"graph\"].to(device)[0]  # [N, N]\n","        graph_data = GCN.process_graph(graph_data)\n","        flow_x = data[\"flow_x\"].to(device)  # [B, N, H, D]\n","        B, N = flow_x.size(0), flow_x.size(1)\n","        flow_x = flow_x.view(B, N, -1)  # [B, N, H*D]  H = 6, D = 1\n","\n","        output_1 = self.linear_1(flow_x)  # [B, N, hid_C]\n","        output_1 = self.act(torch.matmul(graph_data, output_1))  # [N, N], [B, N, Hid_C]\n","\n","        output_2 = self.linear_2(output_1)\n","        output_2 = self.act(torch.matmul(graph_data, output_2))  # [B, N, 1, Out_C]\n","\n","        return output_2.unsqueeze(2)\n","\n","    @staticmethod\n","    def process_graph(graph_data):\n","        N = graph_data.size(0)\n","        matrix_i = torch.eye(N, dtype=graph_data.dtype, device=graph_data.device)\n","        graph_data += matrix_i  # A~ [N, N]\n","\n","        degree_matrix = torch.sum(graph_data, dim=-1, keepdim=False)  # [N]\n","        degree_matrix = degree_matrix.pow(-1)\n","        degree_matrix[degree_matrix == float(\"inf\")] = 0.  # [N]\n","\n","        degree_matrix = torch.diag(degree_matrix)  # [N, N]\n","\n","        return torch.mm(degree_matrix, graph_data)  # D^(-1) * A = \\hat(A)\n","\n","\n","# 图注意力层的定义\n","class GraphAttentionLayer(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super(GraphAttentionLayer, self).__init__()\n","        self.in_c = in_c\n","        self.out_c = out_c\n","\n","        self.F = F.softmax\n","\n","        self.W = nn.Linear(in_c, out_c, bias=False)  # y = W * x\n","        self.b = nn.Parameter(torch.Tensor(out_c))\n","\n","        nn.init.normal_(self.W.weight)\n","        nn.init.normal_(self.b)\n","\n","    def forward(self, inputs, graph):\n","        \"\"\"\n","        :param inputs: input features, [B, N, C].\n","        :param graph: graph structure, [N, N].\n","        :return:\n","            output features, [B, N, D].\n","        \"\"\"\n","\n","        h = self.W(inputs)  # [B, N, D]\n","        outputs = torch.bmm(h, h.transpose(1, 2)) * graph.unsqueeze(0)  # [B, N, N]      x(i)^T * x(j)\n","        outputs.data.masked_fill_(torch.eq(outputs, 0), -float(1e16))   # x(i)|| x(j)\n","\n","        attention = self.F(outputs, dim=2)   # [B, N, N]\n","        return torch.bmm(attention, h) + self.b  # [B, N, N] * [B, N, D]\n","\n","\n","class GATSubNet(nn.Module):\n","    def __init__(self, in_c, hid_c, out_c, n_heads):\n","        super(GATSubNet, self).__init__()\n","\n","        self.attention_module = nn.ModuleList([GraphAttentionLayer(in_c, hid_c) for _ in range(n_heads)])\n","        self.out_att = GraphAttentionLayer(hid_c * n_heads, out_c)\n","\n","        self.act = nn.LeakyReLU()\n","\n","    def forward(self, inputs, graph):\n","        \"\"\"\n","        :param inputs: [B, N, C]\n","        :param graph: [N, N]\n","        :return:\n","        \"\"\"\n","        outputs = torch.cat([attn(inputs, graph) for attn in self.attention_module], dim=-1)  # [B, N, hid_c * h_head]\n","        outputs = self.act(outputs)\n","\n","        outputs = self.out_att(outputs, graph)\n","\n","        return self.act(outputs)\n","\n","\n","\n","# GAT 网络的定义\n","class GATNet(nn.Module):\n","    def __init__(self, in_c, hid_c, out_c, n_heads):\n","        super(GATNet, self).__init__()\n","        self.subnet = GATSubNet(in_c, hid_c, out_c, n_heads)\n","\n","    def forward(self, data, device):\n","        graph = data[\"graph\"][0].to(device)  # [N, N]\n","        flow = data[\"flow_x\"]  # [B, N, T, C]\n","        flow = flow.to(device)\n","\n","        B, N = flow.size(0), flow.size(1)\n","        flow = flow.view(B, N, -1)  # [B, N, T * C]\n","\n","\n","        prediction = self.subnet(flow, graph).unsqueeze(2)  # [B, N, 1, C]\n","\n","        return prediction"],"metadata":{"id":"pmkii296bJp0","executionInfo":{"status":"ok","timestamp":1690039675182,"user_tz":-420,"elapsed":306,"user":{"displayName":"Hà Dung - Hoàng Anh","userId":"11283816701094956411"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def get_adjacent_matrix(distance_file: str, num_nodes: int, id_file: str = None, graph_type=\"connect\") -> np.array:\n","    \"\"\"\n","    :param distance_file: str, 用于保存节点之间距离的文件\n","    :param num_nodes: int, number of nodes in the graph\n","    :param id_file: str, 保存节点之间绝对顺序的文件.\n","    :param graph_type: str, [\"connect\", \"distance\"] ，可以选择是否使用距离作为边的权重\n","    :return:  邻接矩阵A ，np.array(N, N)\n","    \"\"\"\n","    A = np.zeros([int(num_nodes), int(num_nodes)])\n","\n","    if id_file:\n","        with open(id_file, \"r\") as f_id:\n","            node_id_dict = {int(node_id): idx for idx, node_id in enumerate(f_id.read().strip().split(\"\\n\"))}\n","\n","            with open(distance_file, \"r\") as f_d:\n","                f_d.readline()\n","                reader = csv.reader(f_d)\n","                for item in reader:\n","                    if len(item) != 3:\n","                        continue\n","                    i, j, distance = int(item[0]), int(item[1]), float(item[2])\n","                    if graph_type == \"connect\":\n","                        A[node_id_dict[i], node_id_dict[j]] = 1.\n","                        A[node_id_dict[j], node_id_dict[i]] = 1.\n","                    elif graph_type == \"distance\":\n","                        A[node_id_dict[i], node_id_dict[j]] = 1. / distance\n","                        A[node_id_dict[j], node_id_dict[i]] = 1. / distance\n","                    else:\n","                        raise ValueError(\"graph type is not correct (connect or distance)\")\n","        return A\n","\n","    with open(distance_file, \"r\") as f_d:\n","        f_d.readline()\n","        reader = csv.reader(f_d)\n","        for item in reader:\n","            if len(item) != 3:\n","                continue\n","            i, j, distance = int(item[0]), int(item[1]), float(item[2])\n","\n","            if graph_type == \"connect\":\n","                A[i, j], A[j, i] = 1., 1.\n","            elif graph_type == \"distance\":\n","                A[i, j] = 1. / distance\n","                A[j, i] = 1. / distance\n","            else:\n","                raise ValueError(\"graph type is not correct (connect or distance)\")\n","\n","    return A\n","\n","\n","def get_flow_data(flow_file: str) -> np.array:\n","    \"\"\"\n","    :param flow_file: str,交通流量数据的 .npz 文件路径\n","    :return: np.array(N, T, D)\n","    \"\"\"\n","    data = pd.read_csv(flow_file).set_index('datetime')\n","    data.index = np.arange(1, len(data)+1)\n","    flow_data = np.array(data.astype(float)).reshape(len(data),65,1).transpose([1, 0, 2])[:, :, 0][:, :, np.newaxis]\n","    return flow_data\n","\n","\n","class LoadData(Dataset):\n","    def __init__(self, data_path, num_nodes, divide_days, time_interval, history_length, train_mode):\n","        \"\"\"\n","        :param data_path: list, [\"graph file name\" , \"flow data file name\"], path to save the data file names.\n","        :param num_nodes: int, number of nodes.\n","        :param divide_days: list, [ days of train data, days of test data], list to divide the original data.\n","        :param time_interval: int, time interval between two traffic data records (mins).\n","        :param history_length: int, length of history data to be used.\n","        :param train_mode: list, [\"train\", \"test\"].\n","        \"\"\"\n","\n","        self.data_path = data_path\n","        self.num_nodes = num_nodes\n","        self.train_mode = train_mode\n","        self.train_days = divide_days[0]  # 45\n","        self.test_days = divide_days[1]  # 14\n","        self.history_length = history_length  # 6\n","        self.time_interval = time_interval  # 5 min\n","\n","        self.one_day_length = int(24 * 60 / self.time_interval)\n","\n","        self.graph = get_adjacent_matrix(distance_file=data_path[0], num_nodes=num_nodes)\n","\n","        self.flow_norm, self.flow_data = self.pre_process_data(data=get_flow_data(data_path[1]), norm_dim=1)\n","\n","    def __len__(self):\n","        \"\"\"\n","        :return: length of dataset (number of samples).\n","        \"\"\"\n","        if self.train_mode == \"train\":\n","            return self.train_days * self.one_day_length - self.history_length\n","        elif self.train_mode == \"test\":\n","            return self.test_days * self.one_day_length\n","        else:\n","            raise ValueError(\"train mode: [{}] is not defined\".format(self.train_mode))\n","\n","    def __getitem__(self, index):  # (x, y), index = [0, L1 - 1]\n","        \"\"\"\n","        :param index: int, range between [0, length - 1].\n","        :return:\n","            graph: torch.tensor, [N, N].\n","            data_x: torch.tensor, [N, H, D].\n","            data_y: torch.tensor, [N, 1, D].\n","        \"\"\"\n","        if self.train_mode == \"train\":\n","            index = index\n","        elif self.train_mode == \"test\":\n","            index += self.train_days * self.one_day_length\n","        else:\n","            raise ValueError(\"train mode: [{}] is not defined\".format(self.train_mode))\n","\n","        data_x, data_y = LoadData.slice_data(self.flow_data, self.history_length, index, self.train_mode)\n","        data_x = LoadData.to_tensor(data_x)  # [N, H, D]\n","        data_y = LoadData.to_tensor(data_y).unsqueeze(1)  # [N, 1, D]\n","\n","        return {\"graph\": LoadData.to_tensor(self.graph), \"flow_x\": data_x, \"flow_y\": data_y}\n","\n","\n","    @staticmethod\n","    def slice_data(data, history_length, index, train_mode):\n","        \"\"\"\n","        :param data: np.array, normalized traffic data.\n","        :param history_length: int, length of history data to be used.\n","        :param index: int, index on temporal axis.\n","        :param train_mode: str, [\"train\", \"test\"].\n","        :return:\n","            data_x: np.array, [N, H, D].\n","            data_y: np.array [N, D].\n","        \"\"\"\n","        if train_mode == \"train\":\n","            start_index = index\n","            end_index = index + history_length\n","        elif train_mode == \"test\":\n","            start_index = index - history_length\n","            end_index = index\n","        else:\n","            raise ValueError(\"train model {} is not defined\".format(train_mode))\n","        data_x = data[:, start_index: end_index]\n","        data_y = data[:, end_index]\n","        return data_x, data_y\n","\n","    @staticmethod\n","    def pre_process_data(data, norm_dim):\n","        \"\"\"\n","        :param data: np.array, original traffic data without normalization.\n","        :param norm_dim: int, normalization dimension.\n","        :return:\n","            norm_base: list, [max_data, min_data], data of normalization base.\n","            norm_data: np.array, normalized traffic data.\n","        \"\"\"\n","        norm_base = LoadData.normalize_base(data, norm_dim)  # find the normalize base\n","        norm_data = LoadData.normalize_data(norm_base[0], norm_base[1], data)  # normalize data\n","        return norm_base, norm_data\n","\n","    @staticmethod\n","    def normalize_base(data, norm_dim):\n","        \"\"\"\n","        :param data: np.array, original traffic data without normalization.\n","        :param norm_dim: int, normalization dimension.\n","        :return:\n","            max_data: np.array\n","            min_data: np.array\n","        \"\"\"\n","        max_data = np.max(data, norm_dim, keepdims=True)  # [N, T, D] , norm_dim=1, [N, 1, D]\n","        min_data = np.min(data, norm_dim, keepdims=True)\n","        return max_data, min_data\n","\n","    @staticmethod\n","    def normalize_data(max_data, min_data, data):\n","        \"\"\"\n","        :param max_data: np.array, max data.\n","        :param min_data: np.array, min data.\n","        :param data: np.array, original traffic data without normalization.\n","        :return:\n","            np.array, normalized traffic data.\n","        \"\"\"\n","        mid = min_data\n","        base = max_data - min_data\n","        normalized_data = (data - mid) / base\n","\n","        return normalized_data\n","\n","    @staticmethod\n","    def recover_data(max_data, min_data, data):\n","        \"\"\"\n","        :param max_data: np.array, max data.\n","        :param min_data: np.array, min data.\n","        :param data: np.array, normalized data.\n","        :return:\n","            recovered_data: np.array, recovered data.\n","        \"\"\"\n","        mid = min_data\n","        base = max_data - min_data\n","\n","        recovered_data = data * base + mid\n","\n","        return recovered_data\n","\n","    @staticmethod\n","    def to_tensor(data):\n","        return torch.tensor(data, dtype=torch.float)\n","\n","\n","if __name__ == '__main__':\n","    train_data = LoadData(data_path=[\"/content/drive/MyDrive/Phân tích dữ liệu lớn - DS200.N21/Data/distance.csv\", \"/content/drive/MyDrive/Phân tích dữ liệu lớn - DS200.N21/Data/Traffic/traffic.csv\"], num_nodes=65, divide_days=[867, 220],\n","                          time_interval=60, history_length=24,\n","                          train_mode=\"train\")\n","\n","    print(len(train_data))\n","    print(train_data[0][\"flow_x\"].size())\n","    print(train_data[0][\"flow_y\"].size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E_vlikyBbO7Z","executionInfo":{"status":"ok","timestamp":1690039675863,"user_tz":-420,"elapsed":685,"user":{"displayName":"Hà Dung - Hoàng Anh","userId":"11283816701094956411"}},"outputId":"a9b217fe-5be5-4238-976e-e2fe843e486c"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["20784\n","torch.Size([65, 24, 1])\n","torch.Size([65, 1, 1])\n"]}]},{"cell_type":"code","source":["class RMSELoss(torch.nn.Module):\n","    def __init__(self):\n","        super(RMSELoss,self).__init__()\n","\n","    def forward(self,x,y):\n","        criterion = nn.MSELoss()\n","        loss = torch.sqrt(criterion(x, y))\n","        return loss\n","\n","class MAPELoss(torch.nn.Module):\n","    def __init__(self):\n","        super(MAPELoss,self).__init__()\n","\n","    def forward(self,x,y):\n","        loss = torch.mean(torch.abs((x - y) / x))\n","        return loss"],"metadata":{"id":"78SQKo12RrQS","executionInfo":{"status":"ok","timestamp":1690039675864,"user_tz":-420,"elapsed":4,"user":{"displayName":"Hà Dung - Hoàng Anh","userId":"11283816701094956411"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def main():\n","    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","\n","    # Loading Dataset\n","    train_data = LoadData(data_path=[\"/content/drive/MyDrive/Phân tích dữ liệu lớn - DS200.N21/Data/distance.csv\", \"/content/drive/MyDrive/Phân tích dữ liệu lớn - DS200.N21/Data/Traffic/traffic.csv\"], num_nodes=65, divide_days=[867, 220],\n","                          time_interval=60, history_length=24,\n","                          train_mode=\"train\")\n","\n","    train_loader = DataLoader(train_data, batch_size=64, shuffle=True, num_workers=65)\n","\n","    test_data = LoadData(data_path=[\"/content/drive/MyDrive/Phân tích dữ liệu lớn - DS200.N21/Data/distance.csv\", \"/content/drive/MyDrive/Phân tích dữ liệu lớn - DS200.N21/Data/Traffic/traffic.csv\"], num_nodes=65, divide_days=[867, 61],\n","                         time_interval=60, history_length=24,\n","                         train_mode=\"test\")\n","\n","    test_loader = DataLoader(test_data, batch_size=64, shuffle=True, num_workers=65)\n","\n","\n","\n","\n","    # Loading Model\n","\n","    # 可以选择底层实现的GCN或ChebNet model.\n","    # 关于模型的解释：可以参考本人知乎链接：\n","    # ChebNet：https://zhuanlan.zhihu.com/p/138420723\n","    # GCN：https://zhuanlan.zhihu.com/p/138686535\n","\n","    model = GCN(in_c=24 , hid_c=1 ,out_c=1)\n","    #model = ChebNet(in_c=6, hid_c=32, out_c=1, K=2)      # 2阶切比雪夫模型\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model = model.to(device)\n","    criterion = nn.MSELoss()\n","    loss_RMSE = RMSELoss()\n","    loss_MAPE = MAPELoss()\n","    loss_MSE = torch.nn.MSELoss()\n","    loss_L1 = torch.nn.L1Loss()\n","    optimizer = optim.Adam(params=model.parameters())\n","\n","    # Train model\n","    Epoch = 10\n","\n","    model.train()\n","    for epoch in range(Epoch):\n","        epoch_loss = 0.0\n","        start_time = time.time()\n","        for data in train_loader:  # [\"graph\": [B, N, N] , \"flow_x\": [B, N, H, D], \"flow_y\": [B, N, 1, D]]\n","            model.zero_grad()\n","            predict_value = model(data, device).to(torch.device(\"cpu\"))  # [0, 1] -> recover\n","            loss = criterion(predict_value, data[\"flow_y\"])\n","            epoch_loss += loss.item()\n","            loss.backward()\n","            optimizer.step()\n","        end_time = time.time()\n","\n","        print(\"Epoch: {:04d}, Loss: {:02.4f}, Time: {:02.2f} mins\".format(epoch, 1000 * epoch_loss / len(train_data),\n","                                                                          (end_time-start_time)/60))\n","\n","    # Test Model\n","    model.eval()\n","    losses_rmse = []\n","    losses_mse = []\n","    losses_mape = []\n","    losses_l1 = []\n","    with torch.no_grad():\n","\n","        total_loss = 0.0\n","        for data in test_loader:\n","            predict_value = model(data, device).to(torch.device(\"cpu\"))  # [B, N, 1, D]\n","            loss = criterion(predict_value, data[\"flow_y\"])\n","            loss_RMSE = RMSELoss()\n","            loss_MAPE = MAPELoss()\n","            loss_MSE = torch.nn.MSELoss()\n","            loss_L1 = torch.nn.L1Loss()\n","\n","            loss_rmse = loss_RMSE(predict_value, data[\"flow_y\"])\n","            loss_mape = loss_MAPE(predict_value, data[\"flow_y\"])\n","            loss_mse = loss_MSE(predict_value, data[\"flow_y\"])\n","            loss_l1 = loss_L1(predict_value, data[\"flow_y\"])\n","\n","            losses_rmse.append(loss_rmse)\n","            losses_mse.append(loss_mse)\n","            losses_mape.append(loss_mape)\n","            losses_l1.append(loss_l1)\n","\n","            total_loss += loss.item()\n","\n","        losses_l1 = np.array(losses_l1)\n","        losses_mse = np.array(losses_mse)\n","        mean_mse = np.mean(losses_mse)\n","\n","        losses_rmse = np.array(losses_rmse)\n","        mean_rmse = np.mean(losses_rmse)\n","\n","        losses_mape = np.array(losses_mape)\n","        mean_mape = np.mean(losses_mape)\n","\n","        mean_l1 = np.mean(losses_l1)\n","        std_l1 = np.std(losses_l1)\n","\n","        print('Tested: MAE: {}, MAPE : {}, MSE: {}, RMSE: {}'.format(mean_l1, mean_mape, mean_mse, mean_rmse))\n","        print(\"Test Loss: {:02.4f}\".format(1000 * total_loss / len(test_data)))\n","\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mw7c4nC4bjlb","executionInfo":{"status":"ok","timestamp":1690040175026,"user_tz":-420,"elapsed":72835,"user":{"displayName":"Hà Dung - Hoàng Anh","userId":"11283816701094956411"}},"outputId":"70e55ecd-eb2c-4227-f78f-ebe4fd09f8f3"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 65 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 65 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 0000, Loss: 3.3688, Time: 0.13 mins\n","Epoch: 0001, Loss: 3.3684, Time: 0.10 mins\n","Epoch: 0002, Loss: 3.3687, Time: 0.10 mins\n","Epoch: 0003, Loss: 3.3683, Time: 0.11 mins\n","Epoch: 0004, Loss: 3.3687, Time: 0.11 mins\n","Epoch: 0005, Loss: 3.3686, Time: 0.12 mins\n","Epoch: 0006, Loss: 3.3686, Time: 0.10 mins\n","Epoch: 0007, Loss: 3.3682, Time: 0.13 mins\n","Epoch: 0008, Loss: 3.3689, Time: 0.10 mins\n","Epoch: 0009, Loss: 3.3687, Time: 0.12 mins\n","Tested: MAE: 0.3348596692085266, MAPE : nan, MSE: 0.17488716542720795, RMSE: 0.4177824556827545\n","Test Loss: 2.7475\n"]}]}]}